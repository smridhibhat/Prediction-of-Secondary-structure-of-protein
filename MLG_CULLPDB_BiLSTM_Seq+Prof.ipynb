{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLG-CULLPDB_BiLSTM-Seq+Prof",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ML in Genomics \n",
        "## BiLSTM-3 Training with Sequence Plus Protein Profiles\n",
        "### Team Members:\n",
        "    1. Hrishikesh Mahajan\n",
        "    2. Pratik Kamble\n",
        "    3. Smridhi Bhat\n",
        "    4. Yash Shekhadar\n"
      ],
      "metadata": {
        "id": "UqHfhiVMRfDP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0I4uel5IQjwv"
      },
      "outputs": [],
      "source": [
        "# Basic Imports\n",
        "import gzip as gz\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from random import random\n",
        "import pickle as pkl\n",
        "\n",
        "# Keras Imports \n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Masking\n",
        "from keras.regularizers import l2\n",
        "from keras.layers import Dropout\n",
        "from keras import optimizers\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = gz.GzipFile(\"/content/drive/MyDrive/MLG/CULLPDB-Train.npy.gz\", \"r\")\n",
        "train_raw = np.load(f)\n",
        "train_data = train_raw.reshape((len(train_raw), 700, 57))\n",
        "r = np.r_[0:21,35:56]\n",
        "train_x = train_data[:, :, r]\n",
        "train_y = train_data[:, :, 22:30]"
      ],
      "metadata": {
        "id": "PvlNdn4IRpgZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GoH_sNIJUy6k",
        "outputId": "a6fc1750-bdbe-4f1c-83af-60f4ad3989f5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5365, 700, 42)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the testing data\n",
        "f = gz.GzipFile(\"/content/drive/MyDrive/MLG/CULLPDB-Test.npy.gz\", \"r\")\n",
        "test_raw = np.load(f)\n",
        "test_data = test_raw.reshape((len(test_raw), 700, 57))\n",
        "r = np.r_[0:21,35:56]\n",
        "test_x = test_data[:, :, r]\n",
        "test_y = test_data[:, :, 22:30]"
      ],
      "metadata": {
        "id": "3i607B6JZqQV"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# Masking to standardize input length\n",
        "model.add(Masking(mask_value=0., input_shape=(700, 42)))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Bidirectional(LSTM(128, return_sequences=True)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "model.add(Dense(8, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',  'mae'])"
      ],
      "metadata": {
        "id": "mQM_Egk1WHql"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding checkpoint logic\n",
        "filepath = '/content/drive/MyDrive/MLG/Model-8/Model-8.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, \n",
        "                             monitor='val_loss',\n",
        "                             verbose=1, \n",
        "                             save_best_only=True,\n",
        "                             mode='min')"
      ],
      "metadata": {
        "id": "h8zY6K-hZEWQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_x, train_y, epochs=40, validation_data = (test_x, test_y), batch_size=32,callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM68JLXOZnUI",
        "outputId": "376f59b4-245d-4b15-8dfc-3ce7ba4f1e91"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.4073 - acc: 0.5175 - mae: 0.1516\n",
            "Epoch 1: val_loss improved from inf to 0.26517, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 41s 140ms/step - loss: 0.4073 - acc: 0.5175 - mae: 0.1516 - val_loss: 0.2652 - val_acc: 0.5931 - val_mae: 0.1307\n",
            "Epoch 2/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.3198 - acc: 0.6336 - mae: 0.1213\n",
            "Epoch 2: val_loss improved from 0.26517 to 0.25042, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.3198 - acc: 0.6336 - mae: 0.1213 - val_loss: 0.2504 - val_acc: 0.6160 - val_mae: 0.1281\n",
            "Epoch 3/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.3023 - acc: 0.6535 - mae: 0.1155\n",
            "Epoch 3: val_loss improved from 0.25042 to 0.23913, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 18s 110ms/step - loss: 0.3023 - acc: 0.6535 - mae: 0.1155 - val_loss: 0.2391 - val_acc: 0.6319 - val_mae: 0.1194\n",
            "Epoch 4/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2871 - acc: 0.6683 - mae: 0.1105\n",
            "Epoch 4: val_loss improved from 0.23913 to 0.22985, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.2871 - acc: 0.6683 - mae: 0.1105 - val_loss: 0.2298 - val_acc: 0.6447 - val_mae: 0.1134\n",
            "Epoch 5/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2743 - acc: 0.6809 - mae: 0.1062\n",
            "Epoch 5: val_loss improved from 0.22985 to 0.22378, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.2743 - acc: 0.6809 - mae: 0.1062 - val_loss: 0.2238 - val_acc: 0.6522 - val_mae: 0.1120\n",
            "Epoch 6/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2682 - acc: 0.6871 - mae: 0.1043\n",
            "Epoch 6: val_loss improved from 0.22378 to 0.22111, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 18s 110ms/step - loss: 0.2682 - acc: 0.6871 - mae: 0.1043 - val_loss: 0.2211 - val_acc: 0.6554 - val_mae: 0.1097\n",
            "Epoch 7/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2632 - acc: 0.6920 - mae: 0.1026\n",
            "Epoch 7: val_loss improved from 0.22111 to 0.21864, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.2632 - acc: 0.6920 - mae: 0.1026 - val_loss: 0.2186 - val_acc: 0.6603 - val_mae: 0.1096\n",
            "Epoch 8/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2598 - acc: 0.6959 - mae: 0.1014\n",
            "Epoch 8: val_loss improved from 0.21864 to 0.21732, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.2598 - acc: 0.6959 - mae: 0.1014 - val_loss: 0.2173 - val_acc: 0.6637 - val_mae: 0.1075\n",
            "Epoch 9/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2550 - acc: 0.7008 - mae: 0.0998\n",
            "Epoch 9: val_loss improved from 0.21732 to 0.21512, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.2550 - acc: 0.7008 - mae: 0.0998 - val_loss: 0.2151 - val_acc: 0.6660 - val_mae: 0.1082\n",
            "Epoch 10/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2514 - acc: 0.7054 - mae: 0.0986\n",
            "Epoch 10: val_loss improved from 0.21512 to 0.21298, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.2514 - acc: 0.7054 - mae: 0.0986 - val_loss: 0.2130 - val_acc: 0.6716 - val_mae: 0.1056\n",
            "Epoch 11/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2479 - acc: 0.7094 - mae: 0.0974\n",
            "Epoch 11: val_loss did not improve from 0.21298\n",
            "168/168 [==============================] - 18s 105ms/step - loss: 0.2479 - acc: 0.7094 - mae: 0.0974 - val_loss: 0.2131 - val_acc: 0.6707 - val_mae: 0.1044\n",
            "Epoch 12/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2443 - acc: 0.7132 - mae: 0.0961\n",
            "Epoch 12: val_loss did not improve from 0.21298\n",
            "168/168 [==============================] - 17s 104ms/step - loss: 0.2443 - acc: 0.7132 - mae: 0.0961 - val_loss: 0.2135 - val_acc: 0.6735 - val_mae: 0.1030\n",
            "Epoch 13/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2422 - acc: 0.7154 - mae: 0.0954\n",
            "Epoch 13: val_loss improved from 0.21298 to 0.21195, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.2422 - acc: 0.7154 - mae: 0.0954 - val_loss: 0.2120 - val_acc: 0.6752 - val_mae: 0.1048\n",
            "Epoch 14/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2394 - acc: 0.7191 - mae: 0.0944\n",
            "Epoch 14: val_loss did not improve from 0.21195\n",
            "168/168 [==============================] - 18s 107ms/step - loss: 0.2394 - acc: 0.7191 - mae: 0.0944 - val_loss: 0.2173 - val_acc: 0.6697 - val_mae: 0.1042\n",
            "Epoch 15/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2386 - acc: 0.7204 - mae: 0.0941\n",
            "Epoch 15: val_loss improved from 0.21195 to 0.21098, saving model to /content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\n",
            "168/168 [==============================] - 19s 112ms/step - loss: 0.2386 - acc: 0.7204 - mae: 0.0941 - val_loss: 0.2110 - val_acc: 0.6766 - val_mae: 0.1041\n",
            "Epoch 16/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2339 - acc: 0.7252 - mae: 0.0926\n",
            "Epoch 16: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.2339 - acc: 0.7252 - mae: 0.0926 - val_loss: 0.2111 - val_acc: 0.6779 - val_mae: 0.1027\n",
            "Epoch 17/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2304 - acc: 0.7288 - mae: 0.0913\n",
            "Epoch 17: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 107ms/step - loss: 0.2304 - acc: 0.7288 - mae: 0.0913 - val_loss: 0.2114 - val_acc: 0.6798 - val_mae: 0.1017\n",
            "Epoch 18/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2272 - acc: 0.7324 - mae: 0.0902\n",
            "Epoch 18: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.2272 - acc: 0.7324 - mae: 0.0902 - val_loss: 0.2131 - val_acc: 0.6789 - val_mae: 0.1015\n",
            "Epoch 19/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2252 - acc: 0.7348 - mae: 0.0895\n",
            "Epoch 19: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 106ms/step - loss: 0.2252 - acc: 0.7348 - mae: 0.0895 - val_loss: 0.2147 - val_acc: 0.6785 - val_mae: 0.1009\n",
            "Epoch 20/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2216 - acc: 0.7386 - mae: 0.0883\n",
            "Epoch 20: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.2216 - acc: 0.7386 - mae: 0.0883 - val_loss: 0.2135 - val_acc: 0.6789 - val_mae: 0.1017\n",
            "Epoch 21/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2188 - acc: 0.7417 - mae: 0.0874\n",
            "Epoch 21: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 106ms/step - loss: 0.2188 - acc: 0.7417 - mae: 0.0874 - val_loss: 0.2160 - val_acc: 0.6794 - val_mae: 0.0992\n",
            "Epoch 22/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2159 - acc: 0.7447 - mae: 0.0863\n",
            "Epoch 22: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.2159 - acc: 0.7447 - mae: 0.0863 - val_loss: 0.2166 - val_acc: 0.6797 - val_mae: 0.0998\n",
            "Epoch 23/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2133 - acc: 0.7475 - mae: 0.0855\n",
            "Epoch 23: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.2133 - acc: 0.7475 - mae: 0.0855 - val_loss: 0.2192 - val_acc: 0.6770 - val_mae: 0.0991\n",
            "Epoch 24/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2102 - acc: 0.7511 - mae: 0.0844\n",
            "Epoch 24: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.2102 - acc: 0.7511 - mae: 0.0844 - val_loss: 0.2226 - val_acc: 0.6750 - val_mae: 0.0994\n",
            "Epoch 25/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2065 - acc: 0.7552 - mae: 0.0831\n",
            "Epoch 25: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.2065 - acc: 0.7552 - mae: 0.0831 - val_loss: 0.2238 - val_acc: 0.6761 - val_mae: 0.0990\n",
            "Epoch 26/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2036 - acc: 0.7586 - mae: 0.0820\n",
            "Epoch 26: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.2036 - acc: 0.7586 - mae: 0.0820 - val_loss: 0.2260 - val_acc: 0.6762 - val_mae: 0.0986\n",
            "Epoch 27/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.2009 - acc: 0.7612 - mae: 0.0811\n",
            "Epoch 27: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.2009 - acc: 0.7612 - mae: 0.0811 - val_loss: 0.2257 - val_acc: 0.6753 - val_mae: 0.0992\n",
            "Epoch 28/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1968 - acc: 0.7660 - mae: 0.0797\n",
            "Epoch 28: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.1968 - acc: 0.7660 - mae: 0.0797 - val_loss: 0.2296 - val_acc: 0.6756 - val_mae: 0.0984\n",
            "Epoch 29/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1936 - acc: 0.7693 - mae: 0.0786\n",
            "Epoch 29: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.1936 - acc: 0.7693 - mae: 0.0786 - val_loss: 0.2334 - val_acc: 0.6729 - val_mae: 0.0990\n",
            "Epoch 30/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1909 - acc: 0.7722 - mae: 0.0776\n",
            "Epoch 30: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.1909 - acc: 0.7722 - mae: 0.0776 - val_loss: 0.2353 - val_acc: 0.6722 - val_mae: 0.0981\n",
            "Epoch 31/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1878 - acc: 0.7752 - mae: 0.0765\n",
            "Epoch 31: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.1878 - acc: 0.7752 - mae: 0.0765 - val_loss: 0.2397 - val_acc: 0.6700 - val_mae: 0.0988\n",
            "Epoch 32/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1839 - acc: 0.7801 - mae: 0.0751\n",
            "Epoch 32: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.1839 - acc: 0.7801 - mae: 0.0751 - val_loss: 0.2441 - val_acc: 0.6688 - val_mae: 0.0980\n",
            "Epoch 33/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1812 - acc: 0.7831 - mae: 0.0741\n",
            "Epoch 33: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 107ms/step - loss: 0.1812 - acc: 0.7831 - mae: 0.0741 - val_loss: 0.2435 - val_acc: 0.6688 - val_mae: 0.0979\n",
            "Epoch 34/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1782 - acc: 0.7867 - mae: 0.0730\n",
            "Epoch 34: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.1782 - acc: 0.7867 - mae: 0.0730 - val_loss: 0.2520 - val_acc: 0.6678 - val_mae: 0.0972\n",
            "Epoch 35/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1768 - acc: 0.7880 - mae: 0.0726\n",
            "Epoch 35: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.1768 - acc: 0.7880 - mae: 0.0726 - val_loss: 0.2482 - val_acc: 0.6671 - val_mae: 0.0975\n",
            "Epoch 36/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1723 - acc: 0.7930 - mae: 0.0710\n",
            "Epoch 36: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.1723 - acc: 0.7930 - mae: 0.0710 - val_loss: 0.2591 - val_acc: 0.6654 - val_mae: 0.0971\n",
            "Epoch 37/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1692 - acc: 0.7968 - mae: 0.0698\n",
            "Epoch 37: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.1692 - acc: 0.7968 - mae: 0.0698 - val_loss: 0.2622 - val_acc: 0.6646 - val_mae: 0.0970\n",
            "Epoch 38/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1666 - acc: 0.7995 - mae: 0.0688\n",
            "Epoch 38: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 108ms/step - loss: 0.1666 - acc: 0.7995 - mae: 0.0688 - val_loss: 0.2676 - val_acc: 0.6638 - val_mae: 0.0972\n",
            "Epoch 39/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1630 - acc: 0.8036 - mae: 0.0675\n",
            "Epoch 39: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.1630 - acc: 0.8036 - mae: 0.0675 - val_loss: 0.2745 - val_acc: 0.6637 - val_mae: 0.0969\n",
            "Epoch 40/40\n",
            "168/168 [==============================] - ETA: 0s - loss: 0.1608 - acc: 0.8061 - mae: 0.0667\n",
            "Epoch 40: val_loss did not improve from 0.21098\n",
            "168/168 [==============================] - 18s 109ms/step - loss: 0.1608 - acc: 0.8061 - mae: 0.0667 - val_loss: 0.2736 - val_acc: 0.6618 - val_mae: 0.0974\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model(\"/content/drive/MyDrive/MLG/Model-8/Model-8.hdf5\")"
      ],
      "metadata": {
        "id": "2lVdR9-HZ-KK"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred = model.predict(np.array([test_x[0]]))"
      ],
      "metadata": {
        "id": "3z-S87Y4wRu8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for a single prediction\n",
        "plt.figure(figsize=(20,4))\n",
        "plt.title(\"True\")\n",
        "plt.imshow(test_y[0][0:67].T)\n",
        "plt.figure(figsize=(20,4))\n",
        "plt.title(\"Predicted\")\n",
        "plt.imshow(test_pred[0][0:67].T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "ScBqYMJJw76V",
        "outputId": "bb552e0f-61ea-4611-ad19-eacd38d5c0ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9b00040cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABHcAAAC0CAYAAAAenMASAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAX/ElEQVR4nO3dfbBtd1kf8O9zzn3PTW7IC4QkFwJDCkSLJN4REWUo1BYF0bEWcbQjtmN0xhfsoBQ7ba2d2mrtKM7oqJQXbauiRrSMI1oRbLEqmAAqkDCJkbyR5OaV3Nzct3PO0z/OVq93crkn/vZ5WTefz8yZs/fa+/esZ6/9W+u39rPXXqu6OwAAAABM08JmJwAAAADA353iDgAAAMCEKe4AAAAATJjiDgAAAMCEKe4AAAAATJjiDgAAAMCEKe4AAE8aVfVzVfUfZ7e/rKo+tUHz7ap6zkbMCwB48lHcAQC2nKr6dFUdqapHq+reWVFm7zzn0d0f7O7nriGX11fVH8xz3gAA86S4AwBsVV/V3XuTXJPkQJJ/c/KDVbVtU7ICANhiFHcAgC2tu+9K8t4knz/7edN3VNXNSW5Okqp6dVV9rKoerqo/rKoX/FXbqrq6qj5SVYeq6peT7DrpsZdV1Z0n3d9fVe+uqvuq6oGq+smqen6Sn0ny4tlRRA/Pnruzqv5rVd0+O7LoZ6pq90mxvq+q7q6qz1TVP1/vZQQAPLkp7gAAW1pV7U/ylUk+Opv0NUlelOSqqro6yTuSfFuSC5P8bJL3zIovO5L8RpL/keSCJL+a5J+cZh6LSX4zyW1JrkhyWZJ3dfeNSb49yR91997uPn/W5IeT/L0kL0zynNnz/90s1iuTfG+SL09yZZJ/OJcFAQBwGoo7AMBW9RuzI2X+IMn/SfKfZtP/c3c/2N1Hklyb5Ge7+0PdvdzdP5/kWJIvnv1tT/KW7j7R3dcl+ZPTzOuLklya5Pu6+3B3H+3uxz3PTlXVbL7/cpbHoVlur5s95bVJ3tndH+/uw0n+/dBSAAA4A79VBwC2qq/p7vedPGG1rpI7Tpr0zCTfXFXfddK0HVkt1HSSu7q7T3rsttPMa3+S27p7aQ15XZxkT5IbZvkkSSVZnN2+NMkNa5gnAMBcOHIHAJiak4s1dyT5oe4+/6S/Pd39S0nuTnJZnVSBSfKM08S8I8kzTnOS5j7l/v1JjiT5vJPmuW928ufM5rt/DfMEAJgLxR0AYMr+W5Jvr6oX1apzqupVVXVukj9KspTku6tqe1V9bVZ/fvV4PpzVoswPz2LsqqqXzB67N8nls3P4pLtXZvP98ap6apJU1WVV9Y9nz/+VJK+vqquqak+SH1iH1w0A8NcUdwCAyeru65N8a5KfTPJQkluSvH722PEkXzu7/2CSr0/y7tPEWU7yVVk9OfLtSe6cPT9J3p/kE0nuqar7Z9P+1Wxef1xVjyR5X5LnzmK9N8lbZu1umf0HAFg39bd/hg4AAADAlDhyBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCtq1H0IsuWOwr9m8fivHIylgOdx69YCxAku13DYcYN4erma3sWJxDIoPmcFG2msOyqOWxGL1QwzlkMEQdWxrPYXl5PMaoObyfrvb3N2phrFa/vG/3eBJbYD2vpXkkMR7ixJ6x96PnsNmeR4yFE5vbPkkuv/S+4Rj33Hz+UPs+dnw4h61g+/PGv9M7cdPgDtoc1Paxfcwk2X3leOc8cvNYHr00hxVkHsNgjW305vF+LO0di7F4fA79cg7LcmXn4Do2hxwW5jAO9uDLOL53fCDd+cgcXsdo+23jr2PxsTms50uD++2D63iSZHA/M0myOBZjZft4Dkt7xpfF5RfdP9R+38J4377hz47d390Xnzp9XYo7V+zfng//zv6hGL93ZGyv9Hs//k+H2ifJJf92OMSweXyQP/rMsZ3aZLyoUUvjg+62I+MFicVDR4faL+/dOZzD6LLc8ZcHh3NYefizwzGGzaHAtHJ07P2ci3kMmDU+WC3sGuubh17x94dzqHnslJ4YC7Lr3iPDOcyjIH7wC/cMtT+xdziFnNg3/obsum+sf59z9/i2/4d/8K3DMX70y18z1H7pL28bzmEreNrPnTsc494vOTSHTMZsu+TS4Rif/4vj3+B9/KsuG2q/fM+9wzn0HMbS2rFjqP3ipZcM5/DAS54+1P7cT4/vDywcH1+Wh559zlD70S8hk2T3/ePFhKU9Y+PgXS8d/4j5zPceG44xus997Cnjr+O8Pxv/giL3PzTWftv466hzxr8EXNk3tn4ce9pY+yQ5ePXY9i5JfuRb3zHU/lV7xrdXi0+/5XF3TPwsCwAAAGDCFHcAAAAAJkxxBwAAAGDC1lTcqapXVtWnquqWqnrzeicFAAAAwNqcsbhTVYtJfirJVyS5Ksk3VNVV650YAAAAAGe2liN3vijJLd19a3cfT/KuJF+9vmkBAAAAsBZrKe5cluSOk+7fOZv2t1TVtVV1fVVdf98D45cSBAAAAODM5nZC5e5+a3cf6O4DF1+4OK+wAAAAAHwOaynu3JVk/0n3L59NAwAAAGCTraW48ydJrqyqZ1XVjiSvS/Ke9U0LAAAAgLXYdqYndPdSVX1nkt9JspjkHd39iXXPDAAAAIAzOmNxJ0m6+7eS/NY65wIAAADAEzS3EyoDAAAAsPEUdwAAAAAmrLp77kHPqwv6RfWKsSALY5dTr8Xxy7H38vJwjFqo4RjDOezYMR5j25p+wXdaK8eODecwFyuD/X0e7+dgv+qlpfEc4BSj63iSZA7b3aqxdWwe68c8tv2pse9OFi++cDyHObyO5QcfHguwModlObg/MLc8OLvMo1/1ymD7+e+DT1Xt3DkWYHT/LnPa9g9a2DW4HJJkcBxNMtw357Is5/CebgW9dGIOQbbAsphDvxr9fD6PfdWawzq28ujhofbz2Fd9X193Q3cfOHW6I3cAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJmzbZidwOrVQYwFG2yepHo+xcO65Q+37GZcM57B03q7hGA89byzGU//wweEc6sHPDsfow4+NBVhcHM5h5dHDYwFqvF+mezwGW8s8+sVoCnNYP7KyMtZ+DjnMY0nW7t1D7R+75pnDOew6OLi9S7I4ujyPHB3OIbvHx7A3/eH7htq/bPdgv9wivuw7v204xp5f//B4IoNj0MKu8T5Rz7x8OMabfuvdQ+1fsuvEcA7ba3yb99jK8aH2Hzh63nAO3/nBbxxq//Tf2T6cw+77xt+PxSNLQ+0fevbY2JEkDz1vfBQ7ce7YOnrurePHD/QcPqVecNPYe3r7a5eHc7jm2bcPx3jNxR8bjjHq9uMXDcd472euGmp/78efOpzDpR8cH8/Pef+NQ+370KHhHE7HkTsAAAAAE6a4AwAAADBhijsAAAAAE6a4AwAAADBhijsAAAAAE3bG4k5V7a+qD1TVJ6vqE1X1ho1IDAAAAIAzW8tF5paSvLG7P1JV5ya5oap+t7s/uc65AQAAAHAGZzxyp7vv7u6PzG4fSnJjksvWOzEAAAAAzmwtR+78taq6IsnVST70OI9dm+TaJNmVPXNIDQAAAIAzWfMJlatqb5JfS/I93f3IqY9391u7+0B3H9ienfPMEQAAAIDTWFNxp6q2Z7Ww8wvd/e71TQkAAACAtVrL1bIqyduT3NjdP7b+KQEAAACwVms5cuclSf5ZkpdX1cdmf1+5znkBAAAAsAZnPKFyd/9BktqAXAAAAAB4gtZ8QmUAAAAAth7FHQAAAIAJq+6ee9Dz6oJ+Ub1iLEiN/RJsYef45dgXLr1kOMbyvnOG2h99+p7hHM756B3DMXplZSzAyhz62cL4rwNr12C/WB5cDkmyvDzWfnFxOIV+9NHhGCuHj4zlMLockqTn8H4M5zD/bejfycJYv1jYvWs8hYsvHI7Re8byWNm9fTiHhdsPDsdYeeSRofa17Yy/mt4QK489NhZgq6wfnF0G9xGT6JtbzeAYtvj85wynsHTe+Di47abbxwIszuF79znsq648eniofS+dGM5hHp/lsn1sn2Dl8OAYOC9bYX+3xvtmDX6GqR3j+3jzsHJ4bP2Yh/f1dTd094FTpztyBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJkxxBwAAAGDCFHcAAAAAJmzbZidwWt1DzVeOHh1OYeXWTw/HSNVQ851/Ol5/WxqOMAcry+MxBpdlkuF+BVvW4Dq2cvjweApziLEVzGFrNayPHdvsFGDrMpaffQbHsOVPfGo4hTnsZW6J8eNsMY/PcplHDFb1eO/uwfW8TxwfzuFs58gdAAAAgAlT3AEAAACYMMUdAAAAgAlT3AEAAACYsDUXd6pqsao+WlW/uZ4JAQAAALB2T+TInTckuXG9EgEAAADgiVtTcaeqLk/yqiRvW990AAAAAHgi1nrkzluSvCnJyjrmAgAAAMATdMbiTlW9OsnB7r7hDM+7tqqur6rrT+TY3BIEAAAA4PTWcuTOS5K8pqo+neRdSV5eVf/z1Cd191u7+0B3H9ienXNOEwAAAIDHc8biTnd/f3df3t1XJHldkvd39zete2YAAAAAnNETuVoWAAAAAFvMtify5O7+/SS/vy6ZAAAAAPCEOXIHAAAAYMIUdwAAAAAmTHEHAAAAYMKe0Dl3NlJtG0tt4cpnDedw4xueMhxjxwOLQ+3Pv2k4hRy9sIZjPOXmE0PtD75w+3AOJ/b1cIxz/3Ks/Y5HxnN44AVj78fl13xmOIdXXvLJ4Rgv2vMXQ+33b3tkOIfLt+0cjrEVPLh8bDjG2x8+MNT+V9/x8uEczrlnZTjGnnuOD7Xf8ZnPDuewcuttwzF6aWksQI1vt+eix7d5AGc0uM1b2Dm+P9Bz2N7V4OuofecN55Dzx2Os7B1bngu3HxzOoXaMf3boQ4+O5XDOnvEcLtg3HKMePjQWYHl5OIdeHt/Hq8Wx40rmsY72Y0eGY6w8Otav1nPfypE7AAAAABOmuAMAAAAwYYo7AAAAABOmuAMAAAAwYYo7AAAAABOmuAMAAAAwYYo7AAAAABOmuAMAAAAwYYo7AAAAABOmuAMAAAAwYYo7AAAAABOmuAMAAAAwYYo7AAAAABOmuAMAAAAwYYo7AAAAABNW3T33oOfVBf2iesVQjMXz9w21r717h9onSZaWxmNUDTXvpeXxHBbGckiSrIz1k9qza9NzSJLeu3sswOD7mST12NGh9v3ZQ8M59JEj4zEG14+ew/uZlTmsH2eJ2rZtLMAXPHc4h8UHxvvmyt49Q+17++JwDscvHN9eHfzCnUPtn/7/HhvOobeNf39z21eMvY5zn//gcA5Pe+1twzFWjh0bC7AO+0pTVTvH+kSSLAzGuO2d+4dzuOK7HxqOsXzvwaH2o+PoVlHbdwzHWLzkqYMBxrd3yxeeOxzjni8Z+/xyyR+Pj6OLn3lgOEYfenQwifGxOE+9cDjE0kVjnwe3fXIO48+jh4dj9NKJsQA1vn7UHN7Thd1j+1e177zhHHr3+BiWe+4bar78yCPDKbyvr7uhuw+cOt2ROwAAAAATprgDAAAAMGGKOwAAAAATprgDAAAAMGFrKu5U1flVdV1V3VRVN1bVi9c7MQAAAADObK2XV/mJJL/d3V9XVTuSjF3OBAAAAIC5OGNxp6r2JXlpktcnSXcfT3J8fdMCAAAAYC3W8rOsZyW5L8k7q+qjVfW2qjpnnfMCAAAAYA3WUtzZluSaJD/d3VcnOZzkzac+qaqurarrq+r6Ezk25zQBAAAAeDxrKe7cmeTO7v7Q7P51WS32/C3d/dbuPtDdB7Zn5zxzBAAAAOA0zljc6e57ktxRVc+dTXpFkk+ua1YAAAAArMlar5b1XUl+YXalrFuTfMv6pQQAAADAWq2puNPdH0tyYJ1zAQAAAOAJWss5dwAAAADYohR3AAAAACZMcQcAAABgwtZ6QuUNt3Lk6FD7xX3nDeewfPFFwzEWji8Nte9t4/W3uuvgcIwsr4y1P35iOIU+Z/dwjEPPv2Co/cricAo57+ZDQ+1r23gSC0f3DMdY+ewjYwGOHhvOoYcjbA0Lu3aOB3nOM4aaryyOb2sOf97ThmMs7RrL4+HnjK8fj+1fHo5x5X8fW8+33fXAcA6pGg5x5X85MtR+eDuRZGVpbBxlvn7plg8MxzjcY/sUX//GNw7nsHT3TcMxsjK+rTgb9InjwzGW7xnbV/3s110znMM8XPreu8cC7Nwxn0QG9fGx97RHPzckWX7BFcMxHnnGrqH2T7n+seEcenkO24ke3OPt8Rzmss+9fbD0MNo+Se/aPh7j2PhnmPXiyB0AAACACVPcAQAAAJgwxR0AAACACVPcAQAAAJgwxR0AAACACVPcAQAAAJgwxR0AAACACVPcAQAAAJgwxR0AAACACVPcAQAAAJgwxR0AAACACVPcAQAAAJgwxR0AAACACVPcAQAAAJgwxR0AAACACVPcAQAAAJiw6u75B626L8ltn+MpFyW5f+4zhjH6JVuVvslWpW+yFemXbFX6JluRfjk9z+zui0+duC7FnTOpquu7+8CGzxg+B/2SrUrfZKvSN9mK9Eu2Kn2TrUi/PHv4WRYAAADAhCnuAAAAAEzYZhV33rpJ84XPRb9kq9I32ar0TbYi/ZKtSt9kK9IvzxKbcs4dAAAAAObDz7IAAAAAJmxDiztV9cqq+lRV3VJVb97IecPJquodVXWwqj5+0rQLqup3q+rm2f+nbGaOPPlU1f6q+kBVfbKqPlFVb5hN1zfZVFW1q6o+XFV/OuubPzib/qyq+tBsXP/lqtqx2bny5FNVi1X10ar6zdl9/ZJNV1Wfrqo/r6qPVdX1s2nGczZdVZ1fVddV1U1VdWNVvVjfPDtsWHGnqhaT/FSSr0hyVZJvqKqrNmr+cIqfS/LKU6a9OcnvdfeVSX5vdh820lKSN3b3VUm+OMl3zLaT+iab7ViSl3f3FyR5YZJXVtUXJ/mRJD/e3c9J8lCSf7GJOfLk9YYkN550X79kq/gH3f3Cky4zbTxnK/iJJL/d3c9L8gVZ3X7qm2eBjTxy54uS3NLdt3b38STvSvLVGzh/+Gvd/X+TPHjK5K9O8vOz2z+f5Gs2NCme9Lr77u7+yOz2oawOtpdF32ST9apHZ3e3z/46ycuTXDebrm+y4arq8iSvSvK22f2KfsnWZTxnU1XVviQvTfL2JOnu4939cPTNs8JGFncuS3LHSffvnE2DreJp3X337PY9SZ62mcnw5FZVVyS5OsmHom+yBcx++vKxJAeT/G6Sv0jycHcvzZ5iXGczvCXJm5KszO5fGP2SraGT/O+quqGqrp1NM56z2Z6V5L4k75z9nPVtVXVO9M2zghMqw+Po1cvIuZQcm6Kq9ib5tSTf092PnPyYvslm6e7l7n5hksuzejTu8zY5JZ7kqurVSQ529w2bnQs8ji/t7muyekqK76iql578oPGcTbItyTVJfrq7r05yOKf8BEvfnK6NLO7clWT/Sfcvn02DreLeqnp6ksz+H9zkfHgSqqrtWS3s/EJ3v3s2Wd9ky5gdvv2BJC9Ocn5VbZs9ZFxno70kyWuq6tNZ/bn/y7N6Lgn9kk3X3XfN/h9M8utZLYobz9lsdya5s7s/NLt/XVaLPfrmWWAjizt/kuTK2RUMdiR5XZL3bOD84Uzek+SbZ7e/Ocn/2sRceBKanSvi7Ulu7O4fO+khfZNNVVUXV9X5s9u7k3x5Vs8J9YEkXzd7mr7Jhuru7+/uy7v7iqzuV76/u78x+iWbrKrOqapz/+p2kn+U5OMxnrPJuvueJHdU1XNnk16R5JPRN88KtXrU1QbNrOors/rb6MUk7+juH9qwmcNJquqXkrwsyUVJ7k3yA0l+I8mvJHlGktuSvLa7Tz3pMqybqvrSJB9M8uf5m/NH/OusnndH32TTVNULsnqCxcWsfjH0K939H6rq2Vk9YuKCJB9N8k3dfWzzMuXJqqpeluR7u/vV+iWbbdYHf312d1uSX+zuH6qqC2M8Z5NV1QuzehL6HUluTfItmY3t0TcnbUOLOwAAAADMlxMqAwAAAEyY4g4AAADAhCnuAAAAAEyY4g4AAADAhCnuAAAAAEyY4g4AAADAhCnuAAAAAEyY4g4AAADAhP1/rqIhrIq01eQAAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rJiH4rhUxBe_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}